version: "3.8"

services:
  # Ollama service for LLM-based translations
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    env_file:
      - ./env/.env
    entrypoint: ["/bin/sh", "/entrypoint.sh"]
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama-entrypoint.sh:/entrypoint.sh
    restart: always
    networks:
      - translation_network

  # Translation API service
  translation-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: language_translation_api
    ports:
      - "8000:8000"
    env_file:
      - ./env/.env
    volumes:
      - ./models:/app/models
      - ./main.py:/app/main.py
      - ./config.py:/app/config.py
      - ./env:/app/env
      - ./src:/app/src
      - ./schema:/app/schema
      - ./utlis:/app/utlis
    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      - ollama
    restart: always
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    networks:
      - translation_network

networks:
  translation_network:
    driver: bridge

volumes:
  ollama_data:
